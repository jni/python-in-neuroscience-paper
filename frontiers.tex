%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use frontiers.cls nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontier after acceptance.                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 2.0 Generated 2013/09/12 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%

%\documentclass{frontiersENG} % for Engineering articles
\documentclass{frontiersSCNS} % for Science articles
%\documentclass{frontiersMED} % for Medicine articles

\usepackage{url,lineno}
\usepackage{graphicx}
\usepackage{minted}
\linenumbers


% Leave a blank line between paragraphs in stead of using \\

\copyrightyear{2013}
\pubyear{2013}

\def\journal{Neuroinformatics}%%% write here for which journal %%%
\def\DOI{}
\def\articleType{Methods}
\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Nunez-Iglesias {et~al.}} %use et al only if is more than 1 author
\def\Authors{Juan Nunez-Iglesias\,$^{1,2,*}$, Ryan Kennedy\,$^{1,3}$, Stephen M. Plaza\,$^{1}$, Anirban Chakraborty\,$^{4}$ and William T. Katz\,$^1$}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{$^{1}$FlyEM project, HHMI Janelia Farm Research Campus, Ashburn, VA, USA \\
$^{2}$ Present address: Life Sciences Computation Centre, Victorian Life Sciences Computation Initiative, Carlton, VIC, Australia \\
$^{3}$ Department of Computer and Information Science, School of Engineering and Applied Sciences, University of Pennsylvania, Philadelphia, PA, USA \\
$^{4}$ Video Computing Group, Department of Electrical Engineering, University of California at Riverside, Riverside, CA, USA}
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{Juan Nunez-Iglesias}
\def\corrAddress{Life Sciences Computation Centre, Victorian Life Sciences Computation Initiative, Carlton, VIC, Australia}
\def\corrEmail{juan.n@unimelb.edu.au}

% \color{FrontiersColor} Is the color used in the Journal name, in the title, and the names of the sections.


\begin{document}
\onecolumn
\firstpage{1}

\title[learned agglomeration]{Graph-based Active Learning of Agglomeration (GALA): a Python library to segment 2D and 3D neuroimages.}
\author[\firstAuthorLast]{\Authors}
\address{}
\correspondance{}
\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}
\topic{Python in Neuroscience II}% If your article is part of a Research Topic, please indicate here which.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% The sections below are for reference only.
%%%
%%% For Original Research Articles, Clinical Trial Articles, and Technology Reports the following sections are mandatory:
%%% Abstract, Introduction, Material and Methods, Results, and Discussion.
%%% Please note that the Material and Methods section can be placed in any of the following ways: before Results, before Discussion or after Discussion.
%%%
%%% For Clinical Case Studies the following sections are mandatory: Abstract, Introduction, Background, Discussion, and Concluding Remarks.
%%%
%%% For all other article types there are no mandatory sections.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

The aim in high-resolution connectomics is to reconstruct complete neuronal connectivity in a tissue.
Currently, the only technology capable of resolving the smallest neuronal processes is electron microscopy (EM).
Thus, a common approach is to perform (error-prone) automatic segmentation of EM images, followed by manual proofreading by experts to fix errors.
We have developed an algorithm and software library to not only improve the accuracy of the initial automatic segmentation, but also point out the image coordinates where it is likely to have made errors.
Our software, called gala (graph-based active learning of agglomeration), improves the state of the art in agglomerative image segmentation.
It is implemented in Python and makes extensive use of the scientific Python stack (numpy, scipy, networkx, scikit-learn, scikit-image, and others).
We present here the software architecture of the gala library, and discuss several designs that we consider would be generally useful for other segmentation packages.
We also discuss the current limitations of the gala library and how we intend to address them.

%%% Leave the Abstract empty if your article falls under any of the following categories: Editorial Book Review, Commentary, Field Grand Challenge, Opinion or specialty Grand Challenge.
\section{}
%As a primary goal, the abstract should render the general significance and conceptual advance of the work clearly accessible to a broad readership. References should not be cited in the abstract.


\tiny
 \keyFont{ \section{Keywords:} connectomics, python, electron microscopy, image segmentation, machine learning} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}


\section{Introduction}

% For Original Research Articles, Clinical Trial Articles, and Technology Reports the introduction should be succinct, with no subheadings.
%
%For Clinical Case Studies the Introduction should include symptoms at presentation, physical exams and lab results.
%

Connectomics, the elucidation of complete neuronal circuits, requires resolutions as low as 5-10nm to distinguish the smallest neuronal processes, but also fields of view hundreds of micrometers across or more, as neurons can easily span those distances.
This size disparity results in large image volumes of at least 10 gigavoxels and often orders of magnitude larger.
Neurons are visible as distinct regions, or segments, in this 3-dimensional image volume.
Combining an accurate segmentation with the position of pre- and post-synaptic sites in the image \citep{Kreshuk:2011el, Jagadeesh:2013wn}, one can obtain the shapes, locations, and connectivity of all the neurons in an image volume, as has been demonstrated in (REFS: Chklovskii 2013, Helmstaedter 2013).

Various methods have been proposed (and implemented) to go from images to neuronal morphology and connectivity.
(REF) used manual tracing of the midline of neuronal processes, along with manual annotation of synapses, to analyze a neuronal circuit.
(REF) refined this approach by having multiple individuals trace the same neuron, thus allowing them to estimate the error rate of their traces.
Manual tracing, however, cannot scale to the reconstruction of large neuronal circuits (REFS).
An alternate strategy, then, has been to use automatic segmentation of the neurons in the image volume, followed by human proofreading of this segmentation \citep{Chklovskii:2010df}.
Until recently, this proofreading was the rate-limiting step for neuronal reconstruction from EM images, for two reasons:
first, the segmentation algorithms were (and still are) orders of magnitude too inaccurate to reconstruct even a single neuron without errors;
and second, the human proofreaders had to examine every voxel of the image, even if the automatic segmentation is correct.
In response, we developed a new machine learning-based algorithm for image segmentation \citep{NunezIglesias:2013cd} that provides state of the art automatic segmentation accuracy and then directs proofreaders to likely areas of error in the segmentation.
This has dramatically sped up proofreading and reconstruction speed (5-16-fold, in our anecdotal observations).

The algorithm, called GALA (graph-based active learning of agglomeration), works by repeatedly consulting a gold standard segmentation (prepared by human annotators) as it agglomerates sub-segments according to its current best guess.
(Note: throughout this paper, we will use "GALA" to describe the algorithm, and "gala" or "Gala" for the Python library and software.)
It thus accumulates a training dataset used to fit a classifier, which guides subsequent agglomeration decisions.
Furthermore, through the probability output of the classifier, it can estimate its own confidence in whether two segments should be merged, and this estimate can be used for proofreading.

GALA outperformed previous agglomeration methods in EM and natural image segmentation \citep{NunezIglesias:2013cd}, and it remained top-ranked for 8 months in the SNEMI3D challenge, ahead of tools developed specifically for anisotropic data \citep{snemi, Liu:2012ba, Kaynig:2013tv} (REF: Tasdizen paper 2014), comprising about 70 submissions.
See Figure \ref{fig:01} for two segmentation examples obtained with the gala library.

Since we have described the details of the GALA algorithm in detail elsewhere (REF), in this paper we focus on the design aspects of our implementation.
We briefly present the Python application programming interface (API) and the command-line interface (CLI), before delving more deeply into particularly useful design choices, and finally discussing the current limitations of the library and future directions.


\begin{figure}
\begin{center}
\includegraphics[width=18cm]{figure1}
\end{center}
 \textbf{\refstepcounter{figure}\label{fig:01} Figure \arabic{figure}.}{ Two sample automatic segmentations performed with gala. (A) The SNEMI3D test data. (B) Our favorite fuzzball from the Berkeley Segmentation Data Set. }
\end{figure}


\section{API}


\subsection{Python API}

GALA belongs a class of segmentation algorithms called agglomerative algorithms, in which segments are formed by merging smaller segments.
Other examples include mean agglomeration (REF), the graphical models of (REF), and Learning to Agglomerate Superpixel Hierarchies (LASH) (REF), which is most similar to GALA.
Agglomerative methods begin with an initial fine-grained segmentation known as an \emph{oversegmentation} or \emph{superpixel map}.
The superpixel approach allows a massive reduction in computational cost, enabling the use of more sophisticated algorithms in the agglomerative step (REF).
Additionally, it allows the use of different strategies to group pixels and regions, which may have very different properties (REF).

GALA uses machine learning to obtain a merge priority function or policy, which dictates which pair of segments to merge next.
It has three main prerequisites to learn this function:
\begin{itemize}
\item a \emph{superpixel map} (or supervoxel), an initial fine-grained segmentation; and
\item a \emph{gold standard segmentation}, that represents the true segmentation of a training volume; and
\item a pixel-level intensity map, which is optional but required for most features.
    This is usually the \emph{probability of boundary} between segments, but can be other things, such as the probability of the pixel belonging to a glial cell, or to an image of a cat (REF). The map can even be the input image itself.
    Indeed, gala allows multi-channel pixel-level maps that are the concatenation of some or all of the above maps.
\end{itemize}

For the first requirement, we have used the watershed algorithm \citep{Vincent:1991}, but other methods, such as SLIC \citep{Achanta:2012wc} would work.
Gala itself contains an implementation of watershed, although for some parameter sets we just wrap the implementation in \texttt{\small scikit-image}, which is more efficient and works both in 2D and 3D.
Again, the only requirement here is that the input volume is partitioned into some integer-labeled regions, and that these regions do not cross true segment boundaries, at least approximately.
The algorithm that generates this initial oversegmentation is not important.

The second requirement is a completely segmented image to be used as ground truth.
For neuronal EM images, we used ground truth segmentation generated with the open-source Raveler software \citep{raveler}, while a large ground truth body exists for natural images in the Berkeley Segmentation Data Set (BSDS) \citep{MartinFTM01}.
For other images, such as 3D fluorescence microscopy images, generation of ground truth can be a laborious process.
This has indeed become the rate-limiting step in a gala segmentation, so we are moving to eliminate this requirement so that only a subset of ground truth is needed (see discussion).

For the final requirement, we have used Ilastik \citep{ilastik} in our own work on EM images, gPb \citep{globalpb} for natural images, or the probability maps provided by \cite{Ciresan:2012vi} for the SNEMI3D challenge.
Gala is agnostic about the origin of the input probability maps, both theoretically and in this implementation.

Given the above, we create a region adjacency graph, or RAG (implemented in \texttt{\small gala.agglo.Rag}) corresponding to the training superpixel and probability maps, and perform repeated training agglomerations of the superpixels while comparing against the ground truth (\texttt{\small Rag.learn\_agglomerate}).
This produces a training set, to which we can fit a classifier, which will then prioritize merges in a test volume to segment.
These operations are illustrated in the following code snippets, which can be run from gala's \texttt{\small tests/example-data} directory.

First, we import the relevant gala modules and read in the data using the \texttt{\small imio} (image IO) module:

{\small
\begin{minted}[samepage=true]{python}
# imports
from gala import imio, classify, features, agglo, evaluate as ev
# read in training data
ground_truth_train = imio.read_h5_stack('train-gt.lzf.h5')
prob_map_train = imio.read_h5_stack('train-p1.lzf.h5')
watershed_train = imio.read_h5_stack('train-ws.lzf.h5')
\end{minted}
}

Next, we create a feature manager.
These can be concatenated using the \texttt{\small Composite} manager.
Managers are covered in more detail in section \ref{sec:feature-man}.

{\small
\begin{minted}[samepage=true]{python}
# create a feature manager
fm = features.moments.Manager()
fh = features.histogram.Manager()
fc = features.base.Composite(children=[fm, fh])
\end{minted}
}

Using the feature manager, watershed oversegmentation, ground truth segmentation, and probability map, we can create a region adjacency graph \texttt{\small g\_train} and obtain a (features, labels) training dataset.

{\small
\begin{minted}[samepage=true]{python}
g_train = agglo.Rag(watershed_train, prob_map_train, feature_manager=fc)
(X, y, w, merges) = g_train.learn_agglomerate(ground_truth_train, fc)[0]
y = y[:, 0] # gala has 3 truth labeling schemes, pick the first one
\end{minted}
}

With the training dataset, we can train a classifier, using scikit-learn syntax.
Indeed, any scikit-learn classifier can be used here.

{\small
\begin{minted}[samepage=true]{python}
rf = classify.DefaultRandomForest().fit(X, y)
\end{minted}
}

By composing the feature map and the classifier, we obtain a policy: a function whose input is a graph and two nodes (representing segments) and whose output is a number in [0, 1].

{\small
\begin{minted}[samepage=true]{python}
learned_policy = agglo.classifier_probability(fc, rf)
\end{minted}
}

This policy is then used to segment a test (previously unseen) volume.
We agglomerate the superpixels until the classifier returns a merge probability of 0.5, which corresponds to even odds that the merge is a true or false merge.
(This assumes a well-calibrated classifier, meaning that the output corresponds to the probability of a sample feature vector belonging to the "+1" class.
Random forests have been shown to be reasonably well-calibrated (REF).)

{\small
\begin{minted}[samepage=true]{python}
# get the test data and make a RAG with the trained policy
prob_map_test, watershed_test = (map(imio.read_h5_stack,
                        ['test-p1.lzf.h5', 'test-ws.lzf.h5']))
g_test = agglo.Rag(watershed_test, prob_map_test, learned_policy, feature_manager=fc)
g_test.agglomerate(0.5) # best expected segmentation
seg_test1 = g_test.get_segmentation()
\end{minted}
}

Because gala was created as research software, it implements a number of additional agglomerative segmentation algorithms, including mean boundary, oriented mean boundary \citep{Arbelaez:jg}, median boundary, superpixel affinity learning \citep{Ren:2003jg} (which we also call "flat" learning), and LASH \citep{Jain:2011vr}.
These are invoked simply by using a different \texttt{\small merge\_priority\_function} keyword argument, or calling \texttt{\small agglo.Rag.learn\_agglomerate} with different parameters.

The gala API presents a simple tool to obtain state of the art segmentations, and also allows the exploration of a complete set of hierarchical agglomerative segmentation strategies.
Further, because the segmentation strategy is learned, it can be applied with very little modification to many different domains, as demonstrated by its success in natural image segmentation as well as two different kinds of EM data.


\subsection{Segmentation evaluation module}

One of the most generally reusable parts of the gala library is the evaluation module in \texttt{\small gala/evaluate.py}.
It offers efficient implementations of edit distance, Rand Index \citep{Rand:1971uy}, Adjusted Rand Index \citep{Hubert:1985}, Fowlkes-Mallows index \citep{Fowlkes:1983wz}, and Variation of Information (VI) \citep{meila:2005}.

Among these, we focused the most effort on the VI metric for its numerous advantages \citep{meila:2005, NunezIglesias:2013cd}.
VI is an information theoretic measure to compare clusterings or segmentations.
It precisely answers the following question: given the identity of a point in segmentation B, how much information, on average, will I need to know its identity in segmentation A, and vice-versa?
If the two segmentations match perfectly, the answer is 0 bits: no additional information is required if the identity in A is the same as the identity in B in every case.
Formally, it is the sum of the conditional entropy of A given B and the conditional entropy of B given A:

\begin{equation}
VI(A, B) = H(A | B) + H(B | A)
\end{equation}

When A is our automatic segmentation and B is the gold standard, $H(A|B)$ measures the oversegmentation or false splits, and $H(B|A)$ measures the undersegmentation or false merges.

The two components of VI are computed efficiently ($O(n_{\textrm{pixels}})$ time complexity) with the \texttt{\small evaluate.split\_vi} function.

{\small
\begin{minted}[samepage=true]{python}
gt_test = imio.read_h5_stack('test-gt.lzf.h5')
import numpy as np
results = np.vstack((
    ev.split_vi(watershed_test, ground_truth_test),
    ev.split_vi(seg_test1, ground_truth_test)
    ))
print(results)
[[ 0.1845286   1.64774412]
 [ 0.33793257  0.28697057]]
\end{minted}
}

This is interpreted as follows: the undersegmentation VI of the watershed superpixels compared to the ground truth segmentation is 0.18 bits.
That is, the average watershed basin will have slightly over 97\% overlap with one ground truth body and 3\% with another (since $-0.97 \log_2(0.97) - 0.03\log_2(0.03) = 0.19 \approx 0.185)$.
In contrast, its oversegmentation VI is 1.64 bits, which means that most ground truth segments are split into more than 3 watershed pixels.
(The conditional entropy of a perfect $(1/3, 1/3, 1/3)$ split is 1.58 bits. A perfect 50-50 split has an entropy of 1 bit.)

By computing the two conditional entropies at each segmentation threshold, we generate the split VI plot (figure \ref{fig:02}) that we introduced in \cite{NunezIglesias:2013cd}, showing the tradeoff between oversegmentation and undersegmentation.
In this plot, the x-axis is the undersegmentation conditional entropy, measuring false merges, and the y-axis is the corresponding oversegmentation measurement.
Agglomerative segmentations begin somewhere close to the y-axis (lots of oversegmentation but very little undersegmentation).
Then, a correct merge results in a downward move along the plot, while an incorrect merge causes a rightward move.
The goal of a good segmentation algorithm, then, is to get as close as possible to (0, 0), a perfect match between automatic segmentation and ground truth.

This approach is in contrast to the commonly used plot of VI against segmentation threshold (see, e.g., \cite{Andres:2012vp}), which hides the tradeoff information.

\begin{figure}
\begin{center}
\includegraphics[width=85mm]{figure2}
\end{center}
 \textbf{\refstepcounter{figure}\label{fig:02} Figure \arabic{figure}.}{ The split VI plot for the FIBSEM dataset in \cite{NunezIglesias:2013cd}. Lower and to the left is better. The stars indicate the point of lowest VI, and the circles indicate the point at threshold 0.5. }
\end{figure}

\subsection{Command-line interface}

In addition to the flexible Python library interface, we developed a set of command-line scripts to perform common gala functions, such as training and segmenting.
This is the primary way of interacting with gala in a production environment.
The scripts are based on the excellent \texttt{\small argparse} module, so usage can be determined by running the scripts with the -h or --help flags.
Each option can be provided on the command line or through a JSON configuration file format.


\section{Design highlights}

In this section, we focus on a few design elements that we consider essential to gala's success.

\subsection{N-dimensional array support}

Many segmentation libraries assume 2D or 3D data, or provide separate functions for each (see, for example, \cite{slic-website}, or many OpenCV functions).

We instead abstracted away the notion of a neighboring pixel (or voxel) with a \texttt{\small get\_neighbor\_idxs} function that depends only on the pixel coordinates, the shape of the array, and a connectivity parameter.
As all operations in our algorithm depend only on the definition of the local neighborhood, this abstraction made gala dimension-agnostic.
This allowed us to produce segmentations of the Berkeley segmentation dataset \citep{MartinFTM01} and a 3D EM dataset from the same code base.

A great many algorithms in computer vision can be parameterized by neighboring voxels.
Thus, we encourage developers to write these using n-dimensional logic from the start to increase the range of applications of their software.
The numpy library's excellent ndarray object was essential for our n-dimensional support, making gala a prime example of the success of the Python ecosystem for scientific computing.


\subsection{Feature managers and feature caches}
\label{sec:feature-man}

An example of a critical feature when determining the probability that two segments should be merged is the average pixel-level probability of boundary \citep{Ren:2003jg}.
As segments are merged, the shared surface between them and their common neighbors increases, thereby making the average a more reliable estimate of the true probability of a boundary.
Recomputing the mean from scratch, however, results in quadratic time complexity, because we are repeatedly iterating over the same pixels after each merge.
Therefore, a more efficient strategy is to cache a sum of the pixel probabilities and the count of pixels examined.
Then, when two boundaries are combined, their probability sums are added together, as are their counts, and the new average probability can be computed by dividing the new sum by the new count, in constant time.
This caching turns a quadratic operation into a linear one.

The above concept can be generalized to a large class of features.
We found that, in many cases, caching intermediate computations dramatically improved the time complexity of feature computation.
For example, to compute the standard deviation of the pixel probabilities, we must cache the sum of the squared pixel probabilities, along with simple sum and counts.
To compute a histogram, we cache the unnormalised histogram, and each bin is summed when two boundaries are combined.

We therefore devised a single class, which we call a feature manager, that is responsible for defining the cached values, and for computing the feature vector from the cached values.
This has enabled less obvious features, including, for example, some based on the convex hull of the segment.
The convex hull feature manager stores as a cache the convex hull of each node.
When two nodes are merged, the resulting convex hull can be computed faster by starting from the two initial hulls, rather than from the newly formed segment, since these have fewer vertices than the segments themselves.
The manager then uses the hull to compute features such as segment convexity, by comparing the volume of the convex hull to the volume of the segment.

We have strived to make it easy to develop new feature managers, which will be useful as new, more sophisticated 3D segmentation features are developed (REF Jain 2014).
A GitHub pull request creating a new manager can be found at:
\url{https://github.com/jni/ray/pull/51}

\subsection{Classifier abstraction}

Given the vast heterogeneity of our initial feature space, we wished to use a random forest (RF) as our classifier of choice.
\texttt{\small scikit-learn}, the present gold-standard in machine learning Python libraries, did not contain a RF implementation when we started building gala.
We therefore decided to use Vigra, a C++ image analysis and machine learning library with Python bindings.
However, we recognized that a cross-compatible interface across libraries would allow rapid testing of various machine learning techniques.
We therefore built a wrapper around Vigra to match \texttt{\small scikit-learn}'s estimator interface, particularly the \texttt{\small fit()}, \texttt{\small predict()}, and \texttt{\small predict\_proba()} methods.

Because of this, it is trivial to try different classifiers for the learning and agglomeration steps of \texttt{\small gala}.
In particular, we have been able to use the recently vastly improved \texttt{\small RandomForestClassifier} from \texttt{\small scikit-learn} version 0.14 with no code changes.

In short, by using established interfaces, we were able to future-proof our software.
We recommend that anyone looking to build software in the Python ecosystem take a long look at related libraries to match interfaces as closely as possible.


\section{Discussion}

Although we have discussed many of gala's strengths and its positive design aspects, it does currently have limitations, which we describe here, along with potential fixes.

\subsection{Complete gold standard requirement}

As currently implemented, gala requires a fully segmented volume from which to learn.
In our experiments, this has become the major bottleneck when starting segmentations on new data.
Therefore, a priority in gala development going forward is the ability to mask volumes so that partial ground truth can be used.

\subsection{Memory and time inefficiency}

Gala's implementation, based on NetworkX, is slow and has a high memory footprint.
However, many improvements are within easy reach.

Firstly, we currently store feature caches and compute feature vectors as separate arrays.
This results in a huge time overhead for large graphs due to memory allocation, and also in memory usage because of the dictionaries required to store all the separate arrays.
However, because we are performing a hierarchical agglomeration, we know that the number of nodes and edges is bounded by twice the initial number.
Therefore, we can pre-allocate an initial array of shape \texttt{\small (2 * n\_nodes, cache\_size)} for the node feature caches, and similarly for the edges, and use an incremental indexing scheme to keep track of which node or edge in the hierarchy uses which row of the array.

Additionally, the graph currently stores indices to the voxels comprising each node and boundary, which is unnecessary.
Space and time can be saved by keeping only a single voxel and rebuilding nodes using a flood fill.

Finally, we chose the heavy \texttt{\small Graph} class of the NetworkX library for its flexibility and fast node addition and removal.
However, this is ultimately unnecessary: we can store the original supervoxel graph using a much more efficient structure, such as \texttt{\small scipy.sparse.csc\_graph}, and maintain a merge tree.
The graph at any level of the hierarchy can be rapidly constructed from this.

\section{Conclusions}

Like most academic software, gala is a mixture of new algorithms, some good design, and a variety of questionable decisions left over from a time of different priorities.
We wrote this description in the hope that the existing and future functionality, the better parts of the software, and the lessons learned will be of value to the wider research community.
In particular, Don Knuth's famous maxim that ``premature optimization is the root of all evil'' \citep{knuth74opt} should not be taken to extremes: in our case, this has led to time and memory performance issues that have been difficult to resolve.
Still, gala's success in segmenting not only the isotropic EM volume for which it was designed \citep{Glasner:2011uk, NunezIglesias:2013cd}, but also the BSDS natural image dataset and the SNEMI3D anisotropic EM dataset, suggests that it will be useful for some time to come.


%%% Use this if adding the figures directly in the mansucript, if so, please remember to also upload the files when submitting your article
%%% There is no need for adding the file termination, as long as you indicate where the file is saved. In the examples below the files (logo1.jpg and logo2.eps) are in the Frontiers LaTeX folder
%%% If using *.tif files convert them to .jpg or .png

% \textbf{Figure 1.}{ Enter the caption for your figure here.  Repeat as  necessary for each of your figures.}\label{fig:01}% If you don't add the figures in the LaTeX files, please upload them when submitting the article.
%
%%% Frontiers will add the figures at the end of the provisional pdf automatically %%%



\bibliographystyle{frontiersSCNS} % for Science and Engineering articles
%\bibliographystyle{frontiersinMED} % for Medicine articles
\bibliography{frontiers.bib}

\end{document}
